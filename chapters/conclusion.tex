The aim of this chapter is to highlight the main outcomes that I have obtained in my work.
Based on the results achieved in the previous chapter (section \ref{sec:modelling2}), we can conclude that Machine Learning models performed differently for many factors such as the type of model, configuration, and resolution.
An important aspect shown in my work is the importance of the number of ground sensor observations used for training the models. In order to have enhanced estimation, reducing the lack of these is essential.\\
In this work I proposed a solution based on the use of KNN to increase the number of cells with interpolated values. An improved solution to increase the number of ground-truth observations is needed, without recurring to interpolation procedure.
In \acrshort{ml} models performance grows up as the model complexity, turning such systems into “black box” approaches and implying uncertainty in the way they work and come to decisions. 
This becomes a problematic challenge for machine learning systems to be used in critical domains, such as healthcare or the economic aspect.
The use of feature selection supports more efficient debugging for causalities and greater trust in the model.
The selection of features also attempts to clarify the decision of a model by determining the influence of each input variable. 
Scores of feature selection alone may not give a proper comprehension of the model’s reasoning, but it is partially helpful for interpreting the decisions taken.\\
Nevertheless, this case study was not brought to build a proper model for air pollution forecast; instead, it aims to detect how the selection of relevant features could affect the interpretability and the performance of the model. \\
The modelling task will be more properly made by an other collegue in the D-DUST project, by means of Machine Learning and geostatistic PM predictive models.\\
The present study confirmed the findings about the model performance which increases if a feature selection is applied, in particular when we have to deal with a limited sample of data \cite{vabalas2019machine}. 
In addition, what coming out from this is that the more the training sample size is limited, the more an accurate selection of the most weighted variables is needed to increase its performance.
We can say that the feature selection application is necessary but not a sufficient condition to have an increment in the model performance.
\begin{comment}
In this work, so it is highlighted the effect of how the training in \acrshort{ml} should benefit from an accurate selection of variables. 
\end{comment}
Instead of faultless building model with exact predictions, the results in this research pointed more towards having an interpretable model from the covariates chosen. 
One of the future outcomes from this is absolutely the importance of a model sufficiently explained.
Future research on AI should extend the explanations and interpretability of \acrshort{ml} models.
In high-risk applications, AI should not be blind. 
It's needed to dissect a model for proper comprehension and explanation.
For sure models like those could be helpful for the implementation of precise forecast models.  
For instance, this could be used as an important component for making an average through an ensemble technique for more complex and complete model (as it has been already done with the CAMS model).
\par
Finally, this work argued that by using feature selection it's possible to detect which are the main factors affecting the target variable and, possibly, to control them to reduce pollutants effects.\\
The score assumed by each different variable in the case study provides a measure of the influence in ammonia and fine particle formation, aspect which finds also confirmation in literature.
\begin{comment}
Looking forward, further attempts for reducing pollutant formation should be made by procedures actually used.
\end{comment}
